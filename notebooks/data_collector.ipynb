{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "911a91da-44d4-4c87-ad4a-4146380db433",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting statsbombpy\n",
      "  Using cached statsbombpy-1.16.0-py3-none-any.whl.metadata (63 kB)\n",
      "Requirement already satisfied: pandas in c:\\users\\barto\\anaconda3\\envs\\pyspark311\\lib\\site-packages (from statsbombpy) (2.3.0)\n",
      "Requirement already satisfied: requests in c:\\users\\barto\\anaconda3\\envs\\pyspark311\\lib\\site-packages (from statsbombpy) (2.32.4)\n",
      "Collecting requests-cache (from statsbombpy)\n",
      "  Using cached requests_cache-1.2.1-py3-none-any.whl.metadata (9.9 kB)\n",
      "Collecting inflect (from statsbombpy)\n",
      "  Using cached inflect-7.5.0-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting joblib (from statsbombpy)\n",
      "  Downloading joblib-1.5.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting more_itertools>=8.5.0 (from inflect->statsbombpy)\n",
      "  Downloading more_itertools-10.7.0-py3-none-any.whl.metadata (37 kB)\n",
      "Collecting typeguard>=4.0.1 (from inflect->statsbombpy)\n",
      "  Using cached typeguard-4.4.4-py3-none-any.whl.metadata (3.3 kB)\n",
      "Requirement already satisfied: typing_extensions>=4.14.0 in c:\\users\\barto\\anaconda3\\envs\\pyspark311\\lib\\site-packages (from typeguard>=4.0.1->inflect->statsbombpy) (4.14.0)\n",
      "Requirement already satisfied: numpy>=1.23.2 in c:\\users\\barto\\anaconda3\\envs\\pyspark311\\lib\\site-packages (from pandas->statsbombpy) (2.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\barto\\anaconda3\\envs\\pyspark311\\lib\\site-packages (from pandas->statsbombpy) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\barto\\anaconda3\\envs\\pyspark311\\lib\\site-packages (from pandas->statsbombpy) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\barto\\anaconda3\\envs\\pyspark311\\lib\\site-packages (from pandas->statsbombpy) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\barto\\anaconda3\\envs\\pyspark311\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->statsbombpy) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\barto\\anaconda3\\envs\\pyspark311\\lib\\site-packages (from requests->statsbombpy) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\barto\\anaconda3\\envs\\pyspark311\\lib\\site-packages (from requests->statsbombpy) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\barto\\anaconda3\\envs\\pyspark311\\lib\\site-packages (from requests->statsbombpy) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\barto\\anaconda3\\envs\\pyspark311\\lib\\site-packages (from requests->statsbombpy) (2025.4.26)\n",
      "Requirement already satisfied: attrs>=21.2 in c:\\users\\barto\\anaconda3\\envs\\pyspark311\\lib\\site-packages (from requests-cache->statsbombpy) (25.3.0)\n",
      "Collecting cattrs>=22.2 (from requests-cache->statsbombpy)\n",
      "  Using cached cattrs-25.1.1-py3-none-any.whl.metadata (8.4 kB)\n",
      "Requirement already satisfied: platformdirs>=2.5 in c:\\users\\barto\\anaconda3\\envs\\pyspark311\\lib\\site-packages (from requests-cache->statsbombpy) (4.3.8)\n",
      "Collecting url-normalize>=1.4 (from requests-cache->statsbombpy)\n",
      "  Using cached url_normalize-2.2.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Using cached statsbombpy-1.16.0-py3-none-any.whl (16 kB)\n",
      "Using cached inflect-7.5.0-py3-none-any.whl (35 kB)\n",
      "Downloading more_itertools-10.7.0-py3-none-any.whl (65 kB)\n",
      "Using cached typeguard-4.4.4-py3-none-any.whl (34 kB)\n",
      "Downloading joblib-1.5.1-py3-none-any.whl (307 kB)\n",
      "Using cached requests_cache-1.2.1-py3-none-any.whl (61 kB)\n",
      "Using cached cattrs-25.1.1-py3-none-any.whl (69 kB)\n",
      "Using cached url_normalize-2.2.1-py3-none-any.whl (14 kB)\n",
      "Installing collected packages: url-normalize, typeguard, more_itertools, joblib, cattrs, requests-cache, inflect, statsbombpy\n",
      "\n",
      "   ----- ---------------------------------- 1/8 [typeguard]\n",
      "   ---------- ----------------------------- 2/8 [more_itertools]\n",
      "   --------------- ------------------------ 3/8 [joblib]\n",
      "   --------------- ------------------------ 3/8 [joblib]\n",
      "   --------------- ------------------------ 3/8 [joblib]\n",
      "   --------------- ------------------------ 3/8 [joblib]\n",
      "   --------------- ------------------------ 3/8 [joblib]\n",
      "   -------------------- ------------------- 4/8 [cattrs]\n",
      "   -------------------- ------------------- 4/8 [cattrs]\n",
      "   ------------------------- -------------- 5/8 [requests-cache]\n",
      "   ------------------------- -------------- 5/8 [requests-cache]\n",
      "   ------------------------------ --------- 6/8 [inflect]\n",
      "   ---------------------------------------- 8/8 [statsbombpy]\n",
      "\n",
      "Successfully installed cattrs-25.1.1 inflect-7.5.0 joblib-1.5.1 more_itertools-10.7.0 requests-cache-1.2.1 statsbombpy-1.16.0 typeguard-4.4.4 url-normalize-2.2.1\n"
     ]
    }
   ],
   "source": [
    "#!pip install statsbombpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "80952b1b-4bee-4e95-9f7a-234d276521b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import statsbombpy.sb as sb\n",
    "\n",
    "# Defining Constants\n",
    "DEFAULT_SEASON = '2015/2016'\n",
    "TOP5_LEAGUES = ['Italy', 'England', 'Spain', 'Germany', 'France']\n",
    "\n",
    "# Create data directory in current working directory\n",
    "DATA_DIR = os.path.join(os.getcwd(), 'data')\n",
    "    \n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"statsbombpy\")\n",
    "\n",
    "def collect_events_data(league, save_path=DATA_DIR, season_name=DEFAULT_SEASON, save_files=True):\n",
    "    \"\"\"\n",
    "    Retrieves and saves all event data from selected league for specified season.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    league : str or list\n",
    "        League name or list of leagues (e.g. 'Italy', 'England', 'Spain', 'Germany', 'France')\n",
    "    save_path: str, optional\n",
    "        Target file save path (default: 'data' folder in current directory)\n",
    "    season_name: str, optional\n",
    "        Season name (default: '2015/2016')\n",
    "    save_files: bool, optional\n",
    "        Whether to save CSV files (default: True)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        DataFrame with all events\n",
    "    \"\"\"\n",
    "    # Create directory if it doesn't exist\n",
    "    if save_files and not os.path.exists(save_path):\n",
    "        os.makedirs(save_path)\n",
    "        print(f\"Created directory: {save_path}\")\n",
    "    \n",
    "    # Handle single league or list of leagues\n",
    "    if isinstance(league, str):\n",
    "        leagues_to_process = [league]\n",
    "    else:\n",
    "        leagues_to_process = league\n",
    "    \n",
    "    all_events_data = []\n",
    "    \n",
    "    # Process each league separately\n",
    "    for current_league in leagues_to_process:\n",
    "        print(f\"\\nStarting data retrieval from league: {current_league}\")\n",
    "        \n",
    "        # Retrieve league data\n",
    "        try:\n",
    "            free_comps = sb.competitions()\n",
    "            \n",
    "            # Filter selected league\n",
    "            league_data = free_comps[(free_comps['season_name']==season_name) & \n",
    "                               (free_comps['country_name']==current_league)]\n",
    "            \n",
    "            if league_data.empty:\n",
    "                print(f\"No data found for league {current_league} in season {season_name}. Skipping.\")\n",
    "                continue\n",
    "            \n",
    "            competitions = list(league_data['competition_id'])\n",
    "            \n",
    "            # Retrieve match IDs\n",
    "            season_id = league_data['season_id'].iloc[0]\n",
    "            all_matches = pd.concat([sb.matches(competition_id=comp_id, season_id=season_id) \n",
    "                                  for comp_id in competitions])\n",
    "            matches_id = list(all_matches['match_id'])\n",
    "            print(f\"Found {len(matches_id)} matches to analyze\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error retrieving matches for league {current_league}: {str(e)}\")\n",
    "            continue\n",
    "        \n",
    "        # Retrieve event data\n",
    "        event_data = []\n",
    "        \n",
    "        for idx, match_id in enumerate(matches_id):\n",
    "            try:\n",
    "                print(f\"Processing match {idx+1}/{len(matches_id)}\", end='\\r')\n",
    "                \n",
    "                # Get all events for this match\n",
    "                events = sb.events(match_id=match_id)\n",
    "                \n",
    "                # Add match_id to the events for tracking\n",
    "                if not events.empty:\n",
    "                    events['match_id'] = match_id\n",
    "                    event_data.append(events)\n",
    "                                \n",
    "            except Exception as e:\n",
    "                print(f\"\\nError with match {match_id}: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        if event_data:\n",
    "            # Combine data from this league\n",
    "            print(\"\\nCombining data...\")\n",
    "            league_events = pd.concat(event_data, ignore_index=True)\n",
    "            \n",
    "            # Basic data info\n",
    "            print(f\"Total events collected: {len(league_events)}\")\n",
    "            print(f\"Event types found: {league_events['type'].nunique()}\")\n",
    "            print(f\"Most common events:\")\n",
    "            print(league_events['type'].value_counts().head(10))\n",
    "            \n",
    "            # Save file for this league\n",
    "            if save_files:\n",
    "                # Create proper filename\n",
    "                season_str = season_name.replace(\"/\", \"_\")\n",
    "                output_filename = os.path.join(save_path, f'all_events_{current_league}_{season_str}.csv')\n",
    "                league_events.to_csv(output_filename, index=False)\n",
    "                print(f\"Data saved to file: {output_filename}\")\n",
    "            \n",
    "            # Add to collective data\n",
    "            all_events_data.append(league_events)\n",
    "    \n",
    "    # Combine data from all leagues if there's more than one\n",
    "    if len(all_events_data) > 0:\n",
    "        all_events = pd.concat(all_events_data, ignore_index=True)\n",
    "        \n",
    "        # Print summary statistics\n",
    "        print(f\"\\n=== SUMMARY STATISTICS ===\")\n",
    "        print(f\"Total events: {len(all_events)}\")\n",
    "        print(f\"Total matches: {all_events['match_id'].nunique()}\")\n",
    "        print(f\"Event types: {all_events['type'].nunique()}\")\n",
    "        print(f\"\\nTop 15 event types:\")\n",
    "        print(all_events['type'].value_counts().head(15))\n",
    "        \n",
    "        # Save collective file if more than one league was processed\n",
    "        if save_files and len(leagues_to_process) > 1:\n",
    "            season_str = season_name.replace(\"/\", \"_\")\n",
    "            output_filename = os.path.join(save_path, f'all_events_combined_{season_str}.csv')\n",
    "            all_events.to_csv(output_filename, index=False)\n",
    "            print(f\"\\nCollective data saved to file: {output_filename}\")\n",
    "        \n",
    "        return all_events\n",
    "    else:\n",
    "        print(\"No data retrieved.\")\n",
    "        return None\n",
    "\n",
    "def analyze_events_for_xt_model(events_df):\n",
    "    \"\"\"\n",
    "    Analyze events data for xT model preprocessing insights.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    events_df : pd.DataFrame\n",
    "        DataFrame with all events\n",
    "    \"\"\"\n",
    "    if events_df is None or events_df.empty:\n",
    "        print(\"No data to analyze\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\n=== ANALYSIS FOR xT MODEL ===\")\n",
    "    \n",
    "    # Essential columns for xT model\n",
    "    essential_cols = ['type', 'location', 'possession', 'possession_team', 'period', 'minute', 'second']\n",
    "    missing_cols = [col for col in essential_cols if col not in events_df.columns]\n",
    "    if missing_cols:\n",
    "        print(f\"WARNING: Missing essential columns: {missing_cols}\")\n",
    "    \n",
    "    # Event types analysis\n",
    "    print(f\"\\nTotal unique event types: {events_df['type'].nunique()}\")\n",
    "    print(\"Event type distribution:\")\n",
    "    event_counts = events_df['type'].value_counts()\n",
    "    for event_type, count in event_counts.head(20).items():\n",
    "        pct = (count / len(events_df)) * 100\n",
    "        print(f\"  {event_type}: {count:,} ({pct:.1f}%)\")\n",
    "    \n",
    "    # Location data analysis\n",
    "    if 'location' in events_df.columns:\n",
    "        events_with_location = events_df.dropna(subset=['location'])\n",
    "        print(f\"\\nEvents with location data: {len(events_with_location):,} ({len(events_with_location)/len(events_df)*100:.1f}%)\")\n",
    "    \n",
    "    # Possession analysis\n",
    "    if 'possession' in events_df.columns:\n",
    "        possessions = events_df['possession'].nunique()\n",
    "        avg_events_per_possession = len(events_df) / possessions\n",
    "        print(f\"Total possessions: {possessions:,}\")\n",
    "        print(f\"Average events per possession: {avg_events_per_possession:.1f}\")\n",
    "    \n",
    "    # Goals analysis for class imbalance\n",
    "    goals = events_df[events_df['type'] == 'Shot']['shot_outcome'] == 'Goal' if 'shot_outcome' in events_df.columns else 0\n",
    "    if hasattr(goals, 'sum'):\n",
    "        goal_count = goals.sum()\n",
    "        shot_count = len(events_df[events_df['type'] == 'Shot'])\n",
    "        if shot_count > 0:\n",
    "            goal_rate = (goal_count / shot_count) * 100\n",
    "            print(f\"\\nGoals: {goal_count}, Shots: {shot_count}, Goal rate: {goal_rate:.1f}%\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    if not os.path.exists(DATA_DIR):\n",
    "        os.makedirs(DATA_DIR)\n",
    "        print(f\"Created data directory: {DATA_DIR}\")\n",
    "    \n",
    "\n",
    "    \n",
    "    # if events_df is not None:\n",
    "    #     print(f\"\\nRetrieved data for {len(events_df):,} events from {len(TOP5_LEAGUES)} leagues\")\n",
    "        \n",
    "    #     # Analyze data for xT model insights\n",
    "    #     analyze_events_for_xt_model(events_df)\n",
    "        \n",
    "    #     # Save a sample for quick inspection\n",
    "    #     sample_size = min(1000, len(events_df))\n",
    "    #     sample_df = events_df.sample(n=sample_size, random_state=42)\n",
    "    #     sample_filename = os.path.join(DATA_DIR, f'events_sample_{sample_size}.csv')\n",
    "    #     sample_df.to_csv(sample_filename, index=False)\n",
    "    #     print(f\"\\nSample of {sample_size} events saved to: {sample_filename}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4a41b9-8192-4aba-8b83-64fa8df806d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting data retrieval for leagues: Italy, England, Spain, Germany, France\n",
      "\n",
      "Starting data retrieval from league: Italy\n",
      "Found 380 matches to analyze\n",
      "Processing match 107/380"
     ]
    }
   ],
   "source": [
    "# Retrieve data for TOP5 leagues\n",
    "print(f\"Starting data retrieval for leagues: {', '.join(TOP5_LEAGUES)}\")\n",
    "events_df = collect_events_data(\"Italy\", save_path=DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5f2fbe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
